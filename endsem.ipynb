{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5c3c649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from warnings import filterwarnings\n",
    "# filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fbba564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = json.load(open(\"data/train_data.json\"))\n",
    "test = json.load(open(\"data/test_data.json\"))\n",
    "metric_embs = np.load(\"data/metric_name_embeddings.npy\")\n",
    "metric_map = json.load(open(\"data/metric_names.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for embeddings\n",
    "model = SentenceTransformer(\"l3cube-pune/indic-sentence-similarity-sbert\")\n",
    "\n",
    "# Prepare training data\n",
    "X, y = [], []\n",
    "for r in train:\n",
    "    txt = f\"{r['system_prompt']} [SEP] {r['user_prompt']} [SEP] {r['response']}\"\n",
    "    text_emb = model.encode(txt, normalize_embeddings=True)\n",
    "    metric_emb = metric_embs[metric_map.index(r['metric_name'])]\n",
    "    X.append(np.concatenate([text_emb, metric_emb]))\n",
    "    y.append(r['score'])\n",
    "\n",
    "X, y = np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare test data\n",
    "X_test= []\n",
    "for r in test:\n",
    "    txt = f\"{r['system_prompt']} [SEP] {r['user_prompt']} [SEP] {r['response']}\"\n",
    "    text_emb = model.encode(txt, normalize_embeddings=True)\n",
    "    metric_emb = metric_embs[metric_map.index(r['metric_name'])]\n",
    "    X_test.append(np.concatenate([text_emb, metric_emb]))\n",
    "\n",
    "X_test = np.array(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68910a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_response(resp):\n",
    "    if random.random() < 0.5:\n",
    "        return \" \".join(resp.split()[:max(1, len(resp.split())//5)])\n",
    "    elif random.random() > 0.5:\n",
    "        words = resp.split()\n",
    "        random.shuffle(words)\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return \"I don't know.\"\n",
    "\n",
    "augmented = []\n",
    "for r in train:\n",
    "    if random.random() < 0.3:  # 30% augmentation\n",
    "        new_r = r.copy()\n",
    "        if r['response'] is not None: \n",
    "            new_r['response'] = degrade_response(r['response']) \n",
    "            new_r['score'] = random.uniform(0.5, 2.0)\n",
    "        augmented.append(new_r)\n",
    "\n",
    "train_aug = train + augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.52s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.89s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.89s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.84s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.74s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.41s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.81s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.71s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.61s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.43s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.34s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.51s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.10s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.82s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.83s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.90s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.50s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.79s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.47s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.38s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.59s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.35s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.52s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.40s/it]\n",
      " 37%|███▋      | 149/408 [15:16<26:21,  6.10s/it]"
     ]
    }
   ],
   "source": [
    "# Model for embeddings\n",
    "model = SentenceTransformer(\"l3cube-pune/indic-sentence-similarity-sbert\", device='cpu')\n",
    "\n",
    "def encode_in_batches(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = model.encode(batch, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=True)\n",
    "        embeddings.append(emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Prepare training data\n",
    "\n",
    "texts = [f\"{r['system_prompt']} [SEP] {r['user_prompt']} [SEP] {r['response']}\" for r in train_aug]\n",
    "text_embs = encode_in_batches(texts, batch_size=16)\n",
    "\n",
    "metric_embs_mapped = np.array([metric_embs[metric_map.index(r['metric_name'])] for r in train_aug])\n",
    "X = np.concatenate([text_embs, metric_embs_mapped], axis=1)\n",
    "y = np.array([r['score'] for r in train_aug])\n",
    "\n",
    "X, y = np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare test data\n",
    "X_test= []\n",
    "for r in test:\n",
    "    txt = f\"{r['system_prompt']} [SEP] {r['user_prompt']} [SEP] {r['response']}\"\n",
    "    text_emb = model.encode(txt, normalize_embeddings=True)\n",
    "    metric_emb = metric_embs[metric_map.index(r['metric_name'])]\n",
    "    X_test.append(np.concatenate([text_emb, metric_emb]))\n",
    "\n",
    "X_test = np.array(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed228ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bin-based sample weights\n",
    "bins = np.linspace(0, 10, 21)  # 0-0.5,0.5-1,...,10\n",
    "bin_idx = np.digitize(y_train, bins) - 1\n",
    "counts = np.bincount(bin_idx, minlength=len(bins))\n",
    "weights = 1.0 / (counts[bin_idx] + 1e-6)\n",
    "weights /= weights.mean()  # normalize mean weight to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ba1834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.190683126449585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train Ridge regressor\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, y_train, sample_weight=weights)\n",
    "pred = reg.predict(X_val)\n",
    "print(\"RMSE:\", root_mean_squared_error(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9291471011207187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Train Huber regressor\n",
    "reg = HuberRegressor(alpha=1e-3, epsilon=1.35, max_iter=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_val)\n",
    "print(\"RMSE:\", root_mean_squared_error(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abb65673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated RMSE: 0.9182661175727844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Calibrate using Isotonic Regression\n",
    "pred_val = reg.predict(X_val)\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(pred_val, y_val)\n",
    "calibrated_val = iso.transform(pred_val)\n",
    "print(\"Calibrated RMSE:\", root_mean_squared_error(y_val, calibrated_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158f1b4",
   "metadata": {},
   "source": [
    "## Final CSV for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = reg.predict(X_test)\n",
    "pred_test_cal = iso.transform(pred_test)\n",
    "\n",
    "# Save predictions in csv\n",
    "df = pd.DataFrame({'ID': [i+1 for i in range(len(test))], 'score': pred_test_cal})\n",
    "df.to_csv(\"me22b214.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
